# The beehive example configuration

# The node name in the cluster
name = "node-1"

# The RPC address to serve requests
raftAddr = "127.0.0.1:10001"

# The RPC address to serve requests
rpcAddr = "127.0.0.1:10002"

# Give the current node a set of labels, and specify which label names are used to identify
# the location. The scheduler will create a replicas of the shard in a different location 
# based on these location labels to achieve high availability.
labels = [
    ["zone", "default-zone"],
    ["rack", "rack-0"]
]
locationLabels = [
    "zone",
    "rack"
]

# How many shards will be created while bootstrap the cluster.
initShards = 1

# The shard group count.
groups = 1

# The maximum bytes data in mb in a shard
shardCapacityBytes = 96

# Check the interval of shard split in seconds.
shardSplitCheckDuration = 30

# The goroutine number of apply raft log. For performance reasons, the raft log apply 
# is executed asynchronously, so the system uses a fixed-size coroutine pool to apply 
# the raft log of all the shards. Must pow of 2.
applyWorkerCount = 32

# The goroutine number of handle raft events. For performance reasons, the system uses 
# a fixed-size coroutine pool to handle the raft events of all the shards. 
# Must pow of 2.
raftMaxWorkers = 32

# The goroutine number of send raft message, the system sends the raft message to the 
# corresponding goroutine according to the shard ID, each goroutine is responsible for 
# a group of stores, only one tcp connection between two stores.
sendRaftMsgWorkerCount = 8

# Disable split the shard, by default, the system checks all shards, when these
# shards are found to exceed the maximum storage threshold, they will perform split.
disableShardSplit = false

# Disable sync operations every time you write raft log to disk.
disableSyncRaftLog = false

# Use memory to store the Application's KV data, the scheduler collects the
# memory usage of the store node and balances the shards in the cluster, 
# otherwise use disk.
useMemoryAsStorage = false

# How many raft messages in a batch to send to other node
sendRaftBatchSize = 64

# The maximum bytes per proposal, if exceeded this value, application will
# receive `RaftEntryTooLarge` error. Default value is 8 MB.
maxProposalBytes = 8

# Limit the speed and size of snapshots to transfer. To avoid taking up too much
# bandwidth, the snapshot is split into a number of chunks.
# the `maxConcurrencySnapChunks` controls the number of chunks sent concurrently.
# the `snapChunkSize` controls the bytes(mb) of each chunk
maxConcurrencySnapChunks = 8
snapChunkSize = 2

# Limit the write speed per shard 
maxConcurrencyWritesPerShard = 10000

# In all replicas of a shard, when any message that does not receive a replica is exceeded 
# this value, the system will remove the replica and the scheduler will choose a new store 
# to create a new replica. 
# Unit is minute.
maxPeerDownTime = 30

# Reporting the shard information to the scheduler's heartbeat time in seconds.
shardHeartbeatDuration = 2

# Reporting the store information to the scheduler's heartbeat time in seconds.
storeHeartbeatDuration = 10

# If the number of logs of the follower node behind the leader node exceeds this value,
# the transfer leader will not be accepted to this node.
maxAllowTransferLogLag = 2

# Enable raft pre-vote
raftPreVote = true

# Raft tick time interval in ms.
raftTickDuration = 1000

# How many ticks to perform timeout elections.
raftElectionTick = 10

# How many ticks to perform raft headrtbeat.
raftHeartbeatTick = 2

# The maximum bytes in mb per raft message.
raftMaxBytesPerMsg = 4

# The maximum inflight messages in raft append RPC.
raftMaxInflightMsgCount = 32

# Compact raft log time interval in seconds
raftLogCompactDuration = 30

# The leader node periodically compact the replicates the logs that have been copied 
# to the follower node, to prevent this operation from being too frequent, limit the 
# number of raft logs that will be compacted.
raftThresholdCompactLog = 256

# The maximum number of raft logs that the leader node has been applied, if exceeded 
# this value, the leader node will force compact log to last applied raft log index, 
# so the follower node may receive a snapshot.
maxRaftLogCountToForceCompact = 1000

# The maximum bytes in mb of raft logs that the leader node has been applied, if exceeded this 
# value, the leader node will force compact log to last applied raft log index, so the follower 
# node may receive a snapshot.
maxRaftLogBytesToForceCompact = 4

# If the leader node triggers the force compact raft log, the compact index is the last applied 
# raft log index of leader node, to avoid sending snapshots to a smaller delayed follower in the 
# future, set a protected value.
maxRaftLogCompactProtectLag   = 100

[prophet]
# The application and prophet RPC address, send heartbeats, alloc id, watch event, etc. required
rpcAddr = "127.0.0.1:9527"

# Current node is used for store cluster metedata
storeMetadata = true

# The embed etcd client address, required while storeMetadata is true
clientAddr = "127.0.0.1:2371"

# The embed etcd peer address, required while storeMetadata is true
peerAddr = "127.0.0.1:2381"

# The cluster seed node, to join the cluster, required while storeMetadata is true
seed = "127.0.0.1:2371"

# The clusters client address list, required while storeMetadata is false
clusters = [
    "127.0.0.1:2371"
]

# The prophet leader node lease TTL in seconds
leaderLeaseTTL = 5

# The maximum number of connections for rpc.
maxRPCCons = 10

# The maximum connection idle time in hour.
maxRPCConnIdle = 1

# The RPC timeout time in second.
maxRPCTimeout = 10

# How many replicas per shard.
countResourceReplicas = 3

# The maximum retry times of schedule operator.
maxScheduleRetries = 3

# The maximum schedule interval in minute per scheduler.
maxScheduleInterval = 1

# The minimum schedule interval in ms per scheduler.
minScheduleInterval = 10

# The timeout time in minute for waitting a operator completed.
timeoutWaitOperatorComplete = 5

# The maximum freeze time in seconds of a store which has no schedule operator.
maxFreezeScheduleInterval = 30

# The maximum count of transfer replica leader operator.
maxRebalanceLeader = 16

# The maximum count of remove|add replica operator.
maxRebalanceReplica = 12

# The maximum count of replica kind operator.
maxScheduleReplica = 16

# The maximum count of node about snapshot operation, the scheduler will not scheduler these stores.
maxLimitSnapshotsCount = 3

# The minimum rate value base on 100 that the storage already used on a store. 
# The scheduler will not schedule the stores which are high than this value.
minAvailableStorageUsedRate = 80

[metric]
# The prometheus pushgateway address
addr = "127.0.0.1:9091"

# The interval seconds to push the metrics to prometheus pushgateway
interval = 10

# The prometheus job
job = "beehive"

# The instance label, default is os.hostname
instance = ""